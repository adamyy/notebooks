{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5908672e-df3a-472b-bcee-0925d9c67425",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add mvn org.apache.kafka kafka_2.12 2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new repo: repository.io.confluent\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2d4fe9-d2a1-4d36-aee8-70aacddce117",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath config resolver repository.io.confluent https://packages.confluent.io/maven/\n",
    "%classpath add mvn io.confluent kafka-avro-serializer 5.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfc379d-49b7-4578-ba32-2790fda6408b",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add mvn org.apache.avro avro 1.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producer Overview\n",
    "\n",
    "- write messages to Kafka: record user activities, record metrics, storing log messages, communicating asynchronously with other applications, buffering information before writing to a database, etc.\n",
    "\n",
    "## Producing Messages\n",
    "\n",
    "- first create a `ProducerRecord`, which must include the topic and the value, optionally include a key and/or a partition\n",
    "- then the producer serialize the key and value objects to ByteArrays so they can be sent over the network\n",
    "- next, the data is sent to a partitioner; if a partition is specified in the ProducerRecord, then the partitioner performs a no-op; otherwise it chooses a partition for the record, usually based on the ProducerRecord key\n",
    "- once a partition is selected, the producer adds the record to a batch of records that will also be sent to the same topic and partition\n",
    "    - a separate thread is responsible for sending those batches of records to the appropriate Kafka brokers\n",
    "- when the broker receives the messages, it sends back a response; if the messages were successfully written to Kafka, it returns a `RecordMetadata` object with the topic, partition, and the offset of the record within the partition\n",
    "- if the broker failed to write the messages, it will return an error; when the producer receives the error, it may perform retry sending the messages before giving up and returning an error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Snip20200418_1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Kafka Producer\n",
    "\n",
    "- a Kafka producer has three mandatory properties\n",
    "- `bootstrap.servers`: list of `host:port` pairs of brokers that the producer will use to establish initial connection to the Kafka cluster; does not need to include all brokers, since the producer will get more information after the initial connection\n",
    "- `key.serializer`: name of a class that will be used to serialize the keys of the records (`org.apache.kafka.common.serialization.Serializer`)\n",
    "- `value.serializer`: name of a class that will be used to serialize the values of the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala\n",
    "import java.util.Properties\n",
    "import org.apache.kafka.clients.producer.KafkaProducer\n",
    "\n",
    "private val kafkaProps: Properties = new Properties()\n",
    "kafkaProps.put(\"bootstrap.servers\", \"localhost:9092\")\n",
    "kafkaProps.put(\"key.serializer\",\n",
    "               \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "kafkaProps.put(\"value.serializer\",\n",
    "               \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "val producer = new KafkaProducer[String, String](kafkaProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafka_definitive_guide.producer.ProducerFactory"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package kafka_definitive_guide.producer;\n",
    "\n",
    "import java.util.Properties;\n",
    "import org.apache.kafka.clients.producer.ProducerConfig;\n",
    "import org.apache.kafka.clients.producer.KafkaProducer;\n",
    "\n",
    "public class ProducerFactory {\n",
    "    public static KafkaProducer<String, String> makeDefaultProducer() {\n",
    "        Properties kafkaProps = new Properties();\n",
    "        kafkaProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"kafka1:19092\");\n",
    "        kafkaProps.put(ProducerConfig.ACKS_CONFIG, \"all\");\n",
    "        kafkaProps.put(ProducerConfig.RETRIES_CONFIG, 0);\n",
    "        kafkaProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\n",
    "                       \"org.apache.kafka.common.serialization.StringSerializer\");\n",
    "        kafkaProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\n",
    "                       \"org.apache.kafka.common.serialization.StringSerializer\");\n",
    "        return new KafkaProducer<String, String>(kafkaProps);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing Using Apache Avro\n",
    "\n",
    "- Apache Avro is a language-neutral data serialization format\n",
    "- Avro data is described in a language-independent schema, usually described in JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "String schemaString =\n",
    "    \"{\\\"namespace\\\": \\\"customerManagement.avro\\\", \\\"type\\\": \\\"record\\\", \\\"name\\\": \\\"Customer\\\",\\\"fields\\\": [{\\\"name\\\": \\\"id\\\", \\\"type\\\": \\\"int\\\"}, {\\\"name\\\": \\\"name\\\", \\\"type\\\": \\\"string\\\"},{\\\"name\\\": \\\"email\\\", \\\"type\\\": [\\\"null\\\",\\\"string\\\"], \\\"default\\\":\\\"null\\\"}]}\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   285  100     8  100   277    800  27700 --:--:-- --:--:-- --:--:-- 31666\n",
      "{\"id\":1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" \\\n",
    "    --data '{\"schema\": \"{\\\"namespace\\\": \\\"customerManagement.avro\\\", \\\"type\\\": \\\"record\\\", \\\"name\\\": \\\"Customer\\\",\\\"fields\\\": [{\\\"name\\\": \\\"id\\\", \\\"type\\\": \\\"int\\\"}, {\\\"name\\\": \\\"name\\\", \\\"type\\\": \\\"string\\\"},{\\\"name\\\": \\\"email\\\", \\\"type\\\": [\\\"null\\\",\\\"string\\\"], \\\"default\\\":\\\"null\\\"}]}\"}' \\\n",
    "    http://schema-registry:8081/subjects/Customer/versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafka_definitive_guide.producer.Customer"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package kafka_definitive_guide.producer;\n",
    "\n",
    "// Apache Avro Code gen is needed, the below example won't work\n",
    "public class Customer {\n",
    "    public final Integer id;\n",
    "    public final String name;\n",
    "    public Customer(Integer id, String name) {\n",
    "        this.id = id;\n",
    "        this.name = name;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "cannot find symbol",
     "evalue": "cannot find symbol",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mcannot find symbol\u001b[0;0m",
      "\u001b[1;31m  symbol:   class AbstractKafkaSchemaSerDeConfig\u001b[0;0m",
      "\u001b[1;31m  location: package io.confluent.kafka.serializers\u001b[0;0m",
      "\u001b[1;31m import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig\u001b[0;0m",
      "\u001b[1;31m        ^                                                            ^ \u001b[0;0m",
      "\u001b[1;31m\u001b[0;0m",
      "\u001b[1;31mcannot find symbol\u001b[0;0m",
      "\u001b[1;31m  symbol:   variable AbstractKafkaSchemaSerDeConfig\u001b[0;0m",
      "\u001b[1;31m  location: class kafka_definitive_guide.producer.BeakerWrapperClass1261714175Id43be059bc8b34a08b97ff6dbc2c72353\u001b[0;0m",
      "\u001b[1;31m props.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, \"\u001b[0;0m",
      "\u001b[1;31m           ^                             ^                               \u001b[0;0m"
     ]
    }
   ],
   "source": [
    "package kafka_definitive_guide.producer;\n",
    "\n",
    "import java.util.Properties;\n",
    "import java.util.concurrent.Future;\n",
    "import org.apache.kafka.clients.producer.ProducerConfig;\n",
    "import org.apache.kafka.clients.producer.KafkaProducer;\n",
    "import org.apache.kafka.clients.producer.ProducerRecord;\n",
    "import org.apache.kafka.clients.producer.RecordMetadata;\n",
    "import org.apache.kafka.common.serialization.StringSerializer;\n",
    "import org.apache.kafka.common.KafkaException;\n",
    "import io.confluent.kafka.serializers.KafkaAvroSerializer;\n",
    "import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n",
    "import kafka_definitive_guide.producer.Customer;\n",
    "\n",
    "Properties props = new Properties();\n",
    "props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"kafka:19092\");\n",
    "props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n",
    "props.put(ProducerConfig.RETRIES_CONFIG, 0);\n",
    "props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n",
    "props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class);\n",
    "props.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, \"http://schema-registry:8081\");\n",
    "\n",
    "String topic = \"customerContacts\";\n",
    "Customer customer = new Customer(1, \"yifan\");\n",
    "ProducerRecord<String, Customer> record = new ProducerRecord<>(topic, String.valueOf(customer.id), customer);\n",
    "KafkaProducer<String, Customer> producer = null;\n",
    "\n",
    "try {\n",
    "    producer = new KafkaProducer<>(props);\n",
    "    RecordMetadata metadata = producer.send(record).get();\n",
    "    System.out.println(\"topic: \" + metadata.topic() + \", partition: \" + metadata.partition() + \", offset: \" + metadata.offset());\n",
    "} catch (KafkaException e) {\n",
    "    e.printStackTrace();\n",
    "} finally {\n",
    "    producer.close();\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending Messages\n",
    "\n",
    "The producer consists of a pool of buffer space that holds records that haven't been transmitted to the server as well as a background I/O thread that is responsible for turning these records into requests and transmitting them to the cluster.\n",
    "\n",
    "- The `send()` method is asynchronous, when called it adds the records to a buffer of pending record sends and immediately returns\n",
    "- The producer maintains buffers of unsent records for each partition\n",
    "    - these buffers are of a size specified by the `batch.size` config\n",
    "- By default a buffer is available to send immediately even there is additional unused space in the buffer, optionally `linger.ms` can be set to something greater than 0 to instruct the producer to wait before sending a request\n",
    "- `buffer.memory` controls the total amount of memory available to the producer for buffering, if records are sent faster than they can be transmitted then the buffer space will be exhausted, in which case additional `send()` calls will block, for a duration up to `max.block.ms` before it throws `TimeoutException`\n",
    "\n",
    "#### Idempotent Producer \n",
    "\n",
    "- The idempotent producer strengthens Kafka's delivery sematics from at least once to exactly once, in particular producer retries will no longer introducer duplicates\n",
    "- To enable: `enable.idempotence = true`, then `retries` config will default to `Integer.MAX_VALUE` and `acks` will default to `all`\n",
    "- The producer can only guarantee idempotence for messages sent within a single session\n",
    "\n",
    "\n",
    "#### Transactional Producer\n",
    "\n",
    "- The transactional producer allows an application to send messages to multiple partitions and topics atomically\n",
    "- To use the transaction, one must set the `transactional.id` config\n",
    "- Idempotence is automatically enabled along with the producer configs which idempotence depends on\n",
    "- Topics included in the transactions must be configured for durability\n",
    "    - `replication.factor >= 3`\n",
    "    - `min.insync.replicas >= 2`\n",
    "- For transactional guarantees to be realized from end-to-end, the consumers must be configured to read only committed messages\n",
    "- `transational.id` enables transaction recovery across multiple sessions of a single producer instance, and it should be unique to each producer instance running within a partitioned application\n",
    "\n",
    "```java\n",
    "Properties props = new Properties();\n",
    "props.put(\"bootstrap.servers\", \"localhost:9092\");\n",
    "props.put(\"transactional.id\", \"my-transactional-id\");\n",
    "Producer<String, String> producer = new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());\n",
    "\n",
    "producer.initTransactions();\n",
    "\n",
    "try {\n",
    "    producer.beginTransaction();\n",
    "    for (int i = 0; i < 100; i++)\n",
    "        producer.send(new ProducerRecord<>(\"my-topic\", Integer.toString(i), Integer.toString(i)));\n",
    "    producer.commitTransaction();\n",
    "} catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) {\n",
    "    producer.close();  // non recoverable exceptions\n",
    "} catch (KafkaException e) {\n",
    "    producer.abortTransaction(); // for all other exception, abort and retry\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "#### Calling `send()`\n",
    "\n",
    "- _Fire-and-forget_\n",
    "    - send a msg and dont care if it arrives successfully\n",
    "    - most of the time it arrives\n",
    "        - since Kafka is HA\n",
    "        - and producer performs retries\n",
    "- _Synchronous send_\n",
    "    - `send()` returns a Future object\n",
    "    - use `get()` to wait on the future \n",
    "- _Asynchronous send_\n",
    "    - call `send()` with a callback\n",
    "    - callback gets triggered when it receives a response from the Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package kafka_definitive_guide.producer.example;\n",
    "\n",
    "import java.util.concurrent.Future;\n",
    "import org.apache.kafka.clients.producer.*;\n",
    "\n",
    "import kafka_definitive_guide.producer.ProducerFactory;\n",
    "\n",
    "KafkaProducer<String, String> producer = ProducerFactory.makeDefaultProducer();\n",
    "ProducerRecord<String, String> record =\n",
    "    new ProducerRecord<>(\"CustomerCountry\", \"Precision Products\", \"France\");\n",
    "\n",
    "try {\n",
    "    Future<RecordMetadata> future = producer.send(record);\n",
    "    RecordMetadata metadata = future.get();\n",
    "    System.out.println(\"Topic: \" + metadata.topic());\n",
    "    System.out.println(\"Partition: \" + metadata.partition());\n",
    "    System.out.println(\"Offset: \" + metadata.offset());\n",
    "} catch (Exception e) {\n",
    "    e.printStackTrace();\n",
    "} finally {\n",
    "    producer.close();   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala\n",
    "import org.apache.kafka.clients.producer.ProducerRecord\n",
    "val record: ProducerRecord[String, String] =\n",
    "    // topic, key, value\n",
    "    new ProducerRecord(\"CustomerCountry\", \"Precision Products\", \"France\")\n",
    "try {\n",
    "    // doesnt block!\n",
    "    // we ignore errors that may occur while\n",
    "    // sending messages to brokers, or in the brokers themselves\n",
    "    val future = producer.send(record) // return Future[ProducerRecord]\n",
    "    // alternativesly, do producer.send(record).get() to block\n",
    "    val result = future.get()\n",
    "} catch {\n",
    "    // we can still get an exception before sending\n",
    "    // e.g., SerializationException, TimeoutException\n",
    "    case e: Exception => e.printStackTrace()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala\n",
    "import org.apache.kafka.clients.producer.Callback\n",
    "import org.apache.kafka.clients.producer.RecordMetadata\n",
    "\n",
    "class ProducerCallback extends Callback {\n",
    "    override def onCompletion(recordMetadata: RecordMetadata, e: Exception): Unit = {\n",
    "        print(recordMetadata.toString())\n",
    "        if (e != null) {\n",
    "            e.printStackTrace()\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "producer.send(record, new ProducerCallback())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Producers\n",
    "\n",
    "### `acks`\n",
    "\n",
    "- controls how many partitions replicas must receive the record before the producer can consider the write successful\n",
    "    - `acks=0`: producer will not wait for a reply from the broker before assuming the message was sent successfully\n",
    "    - `acks=1`: producer will receive a success response from the broker the moment the leader replica received the message\n",
    "    - `acks=all`: the prodcuer will receive a success response from the broker once all in-sync replicas received the message\n",
    "\n",
    "\n",
    "### `buffer.memory`\n",
    "\n",
    "- sets the amount of memory the producer will use to buffer messages waiting to be sent to brokers\n",
    "- if messages are sent by the application faster than they can be delivered to the server, the producer may run out of space and additional `send()` calls will either block or throw an exception (based on `block.on.buffer.full` parameter)\n",
    "\n",
    "### `compression.type`\n",
    "\n",
    "- `snappy`/`gzip`/`lz4`\n",
    "- default is uncompressed\n",
    "- `snappy` provides decent compression ratios with low CPU overhead and good performance, recommended in cases where both performance and bandwidth are a concern\n",
    "- `gzip` uses more CPU and time but result in better compression ratios\n",
    "\n",
    "### `retries`\n",
    "\n",
    "- how many times the producer will retry sending the message upon a transient error before giving up and notifying the client of an issue\n",
    "    - `retry.backoff.ms`: how long the producer wait in between retries\n",
    "    - backoff should consider the time it takes from recovering from a crashed broker (i.e., how long until all partitions get new leaders)\n",
    "- developers should focus on handling non-retriable errors or cases where retry attempts were exhausted\n",
    "\n",
    "### `batch.size`\n",
    "\n",
    "- when multiple records are sent to the same partition, the producer will batch them together\n",
    "- controls the amount of memory in bytes that will be used for each batch\n",
    "- producer will send half-full batches and will not wait for a batch to become full\n",
    "\n",
    "### `linger.ms`\n",
    "\n",
    "- controls the amount of time to wait for additional messages before sending the current batch\n",
    "- the producer sends a batch of messages either when the current batch is full or when `linger.ms` is reached\n",
    "- increasing `linger.ms` increases latency but also increases throughput since we send more messages at once, there is less overhead per message\n",
    "    \n",
    "### `client.id`\n",
    "\n",
    "- used by the brokers to identify messages sent from the client\n",
    "- used in logging, metrics, and quotas\n",
    "\n",
    "### `max.in.flight.requests.per.connection`\n",
    "\n",
    "- controls how many messages the producer will send to the server without receiving responses\n",
    "- setting this high can increase memory usage while improving throughput\n",
    "- setting this too high can reduce throughput as batch becomes less efficient\n",
    "- setting this to 1 guarantees that the mesages will be written to the broker in the order in which they were sent, even when retries occur\n",
    "\n",
    "### `timeout.ms`, `request.timeout.ms`, `metadata.fetch.timeout.ms`\n",
    "\n",
    "- controls how long the producer will wait for a reply from the server when sending the data (`request.timeout.ms`) and when requesting metadata such as the current leaders for the partitions we are writing to (`metadata.fetch.timeout.ms`)\n",
    "- upon timeout the producer either retry sending or respond with an error (throwing exception or sends callback)\n",
    "- `timeout.ms` controls the time the broker will wait for in-sync replicas to acknowledge the message in order to meet the `acks` configuration\n",
    "    - the broker will return an error if the time elapses without necessary acknowledgements\n",
    "    \n",
    "### `max.block.ms`\n",
    "\n",
    "- controls how long the producer will block when calling `send()` and when explicitly requesting metadata via `partitionsFor()`\n",
    "\n",
    "### `max.request.size`\n",
    "\n",
    "- controls the size of a produce request sent by the producer\n",
    "- caps both the size of the largest message that can be sent and the number of messages that the producer can send in one request\n",
    "- the broker has its own limit on the size of the largest message it will accept (`message.max.bytes`)\n",
    "\n",
    "\n",
    "### `receive.buffer.bytes` & `send.buffer.bytes`\n",
    "\n",
    "- the sizes of the TCP send/receive buffers used by the sockets when writing & reading data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering Guarantees\n",
    "\n",
    "- Kafka preserves the order of messages within a partition\n",
    "- however, having nonzero `retries` parameter and higher-than-one `max.in.flights.requests.per.session` means that it is possible the broker will fail to write the first batch of messages, succeed to write the second batch, then retry the first batch and succeed, thereby reversing the order\n",
    "- setting `retries` to zero is not an option for a reliable system\n",
    "- set `in.flight.requests.per.session` to 1 to make sure that while a batch of messages is retrying, additional messages will not be sent\n",
    "- ^ only use when order is important as the throughput will be severely limited\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions\n",
    "\n",
    "- Kafka messages are key-value pairs; but one can create a `ProducerRecord` with only topic and value, with the key set to null by default\n",
    "- Keys servers two goals: they add additional information that gets stored with the message; and they are used to decide which one of the topic partitions the message will be written to\n",
    "- All messages with the same key will go to the same partition\n",
    "    - When the key is null and the default partitioner is used, the record will be sent to one of the available partitions of the topic at random: a round-robin algorithm will be used to balance the messages among the partitions\n",
    "    - If a key exists and the default partitioner is used, Kafka will hash the key and use the result to map the message to a specific partition\n",
    "        - all partitions are considered during key mapping, so if a specific partition is unavailable when the data is written to it, there might be an error\n",
    "        - the mapping of keys to partitions is consistent only as long as the number of partitions in a topic does not change\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Partitioning Strategy\n",
    "\n",
    "- Example: suppose one type of key is particularly common, and it leads to data skew among partitions and brokers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package kafka_definitive_guide.producer;\n",
    "\n",
    "import java.util.*;\n",
    "import org.apache.kafka.clients.producer.Partitioner;\n",
    "import org.apache.kafka.common.Cluster;\n",
    "import org.apache.kafka.common.PartitionInfo;\n",
    "import org.apache.kafka.common.record.InvalidRecordException;\n",
    "import org.apache.kafka.common.utils.Utils;\n",
    "\n",
    "public class SkewPartitioner implements Partitioner {\n",
    "    public void configure(Map<String, ?> configs) {}\n",
    "    \n",
    "    public int partition(String topic,\n",
    "                         Object key, byte[] keyBytes,\n",
    "                         Object value, byte[] valueBytes,\n",
    "                         Cluster cluster) {\n",
    "        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);\n",
    "        int numPartitions = partitions.size();\n",
    "        \n",
    "        if ((keyBytes == null) || (!(key instanceof String)))\n",
    "            throw new InvalidRecordException(\"All messages are expected to have key\");\n",
    "        \n",
    "        String keyString = (String) key;\n",
    "        // let the very common key to always go to a certain partition, in this case the last one\n",
    "        if (keyString.equals(\"VeryCommonKey\")) return numPartitions; \n",
    "        // other records will get hashed to the rest of the partitions\n",
    "        return (Math.abs(Utils.murmur2(keyBytes)) % (numPartitions - 1));\n",
    "    }\n",
    "    \n",
    "    public void close() {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "text/x-java",
   "file_extension": ".java",
   "mimetype": "",
   "name": "Java",
   "nbconverter_exporter": "",
   "version": "1.8.0_121"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "207px",
    "width": "263px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
