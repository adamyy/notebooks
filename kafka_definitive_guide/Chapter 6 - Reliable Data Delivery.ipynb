{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability Guarantees\n",
    "\n",
    "- Kafka provides order guarantee of messages in a partition\n",
    "- Produced messages are considered committed when they were written to the partition on all its in-sync replicas\n",
    "- Messages that are committed will not be lost as long as at least one replica remains alive\n",
    "- Consumers can only read messages that are committed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication\n",
    "\n",
    "- Each Kafka topic is broken down into *partitions*, which are the basic data building blocks\n",
    "- A partition is tored on a single disk\n",
    "- Kafka guarantees order of events within a partition and a partition can be either online (available) or offline (unavailable)\n",
    "- Each partition can have multiple replicas, one of which is a designated leader\n",
    "    - All events are produced to and consumed from the leader replica\n",
    "    - Other replicas just need to stay in sync with the leader and replicate all the recent events on time\n",
    "    - If the leader becomes unavailable, one of the in-sync replicas becomes the new leader\n",
    "\n",
    "\n",
    "- A follower replica is considered in-sync if it\n",
    "    - Has an active session with Zookeeper, meaning that it sends periodic heartbeat to Zookeeper\n",
    "    - Fetched messages from the leader in the last n seconds (configurable)\n",
    "    - Fetched the most recent messages from the leader in the last n seconds (configurable)\n",
    "\n",
    "\n",
    "- An in-sync replica that is slightly behind can slow down producers and consumers - since they wait for all the in-sync replicas to get the message before it is *committed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broker Configuration \n",
    "\n",
    "- There are three configuration parameters in the broker that change Kafka's behavior regarding reliable message storage\n",
    "    - These configs can be applied at the broker level, which control configuration for all topics in the system\n",
    "    - Or at the topic level, controlling behavior for a specific topic\n",
    "    - This fine granularity of configuration allows the same Kafka cluster to be used to host reliable and non-reliable topics\n",
    "\n",
    "\n",
    "### Replication Factor\n",
    "\n",
    "- Topic level: `replication.factor`, broker level: `default.replication.factor`\n",
    "- A replication factor of $N$ allows you to lose $N-1$ brokers while still being able to to read and write data to the topic reliably\n",
    "    - Higher replication factor leads to higher availability, higher reliability, fewer disasters\n",
    "    - However, you need at least $N$ brokers and $N$ times the disk space\n",
    "    \n",
    "    \n",
    "- By default, Kafka places each replica for a partition on a separate broker, in some cases this is not safe enough if the brokers are on the same rack\n",
    "- To protect against rack-level failures, brokers are recommended to placed in multiple racks and using the `broker.rack` broker configuration to configure the rack name for each broker\n",
    "- If rack names are configured, Kafka ensures that replicas for a partition are spread across multiple racks\n",
    "\n",
    "\n",
    "### Unclean Leader Election\n",
    "\n",
    "- Only available at the broker level (cluster-wide in practice): `unclean.leader.election.enable`, default to true\n",
    "- When the leader for a partition is no longer available, one of the in-sync replicas will be chosen as the new leader\n",
    "    - The leader election is \"clean\" in that it **guarantees no loss of comitted data**\n",
    "\n",
    "\n",
    "- In cases where there are no in-sync follower replicas exist when the leader replica becomes unavailable, we have the option of allowing the out-of-sync replica to become the new leader\n",
    "    - If we don't want such behavior, then the partition remains offline until the old leader is revived, we lose availability\n",
    "    - If we do want such behavior, we will lose all messages that were written to the old leader while that replica was out of sync, risking data loss and data inconsistencies \n",
    "\n",
    "- Typically, unclean leader election is disabled in systems where data quality and consistency are critical, e.g., banking systems\n",
    "- Enabled where availability is more important, e.g., real-time clickstream analysis\n",
    "    \n",
    "### Minimum In-Sync Replicas\n",
    "\n",
    "- Both the topic and the broker level: `min.insync.replicas`\n",
    "- Even though a topic can be configured to have three replicas, you may end up only having one in-sync replica; if this in-sync replica becomes unavailable, you choose between availability and consistency\n",
    "- Also, per Kafka reliability guarantees, data is considered committed when it is written to all in-sync replicas, even when `all` means just one replica\n",
    "\n",
    "\n",
    "- `min.insync.replicas` can be used to ensure that committed data is written to more than one replica\n",
    "- When the number of in-sync replicas falls below this number, the brokers will no longer accept produce requests by responding with `NotEnoughReplicasException`; consumers can continue readig existing data; effectively making the in-sync replica read-only\n",
    "- This prevents the undesirable situation where data is produced and consumed, only to disappear when unclean election occurs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Producers in a Reliable System\n",
    "\n",
    "- The configuration on the producers needs to match the behavior of the brokers in order to guarantee reliability, thus the producers should\n",
    "    - use the correct `acks` configuration to match reliability requirements\n",
    "    - handle errors correctly both in configuration and in code\n",
    "\n",
    "### Sending Acknowledgements\n",
    "\n",
    "- `acks=0`: fastest, a message is considered committed if the producer managed to send it over the network, maximize throughput but guarantee to lose some messages since it won't have knowledge of a unavailable leader or cluster\n",
    "- `acks=1`: requires an ack from the leader, gets `LeaderNotAvailableException` while a leader is getting elected, in which case the producer should retry, can still lose data if the leader crashes and some messages that were successfully written to the leader and acknowledged were not replicated to the followers before the crash\n",
    "- `acks=all`: slowest, safest, the leader will wait until all in-sync replicas got the message before sending back an ack or an error\n",
    "\n",
    "### Configuring Producer Retries\n",
    "\n",
    "- Retriable errors will be handled by the producer API\n",
    "- Non-retriable/non-transient/non-resolvable errors needs to be handled by the developer in code\n",
    "- It is recommended to configure the producer to keep retrying upon a retriable error if you never want to lose a message\n",
    "- Retries and error handling can guarantee that each message will be stored **at least once**, but it is difficult to guarantee *exactly once*\n",
    "\n",
    "### Additional Error Handling\n",
    "\n",
    "- Nonretriable broker errors such as errors regarding message size, authorization errors, etc.\n",
    "- Errors that occur before the message was sent to the broker, e.g., serialization errors\n",
    "- Errors that occur when the producer exhausted all retry attempts or when the available memory used by the producer is filled to the limit due to using all of it to store messages while retrying\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Consumers in a Reliable System\n",
    "\n",
    "- Data is only available to consumers after it has been committed to Kafka (written to all in-sync replicas)\n",
    "    - Consumers get data that is guaranteed to be consistent\n",
    "- When reading data from a partition, a consumer is fetching a batch of events, checking the last offset in the batch, then requesting another batch starting from the last offset received\n",
    "- Consumers commit their offsets in cases when they need resume consumptions from a previous checkpoint\n",
    "    - For each partition it is consuming, the consumer stores its current location\n",
    "    \n",
    "\n",
    "### Consumer Configuration Properties for Reliable Processing\n",
    "\n",
    "- `group.id`: if multiple consumers have the same group ID and subscribe to the same topic, each will be assigned a subset of the partitions in the topic and will therefore only read a subset of the messages individually\n",
    "    - If a consumer needs to consume every single message in the topics, it needs to have a unique `group.id`\n",
    "    \n",
    "    \n",
    "- `auto.offset.reset`: controls the consumer behavior when no offsets were committed (e.g., when the consumer first starts) or when the consumer asks for offsets that don't exist in the broker\n",
    "    - Either `earliest`: start from the beginning of the partition whenever it doesn't have a valid offset; guarantees minimum data loss but leads to the consumer processing messages twice\n",
    "    - Or `latest`: start from the end of the partition; minimizes duplicate processing but almost certainly leads to message loss\n",
    "\n",
    "\n",
    "- `enable.auto.comit`: controls whether the offsets are implicitly committed by the consumer API or explicit committed by the developer\n",
    "    - If all the processing of consumed records are done within the poll loop, then the automatic offset commit guarantees you never commit an offset that has not been processed\n",
    "        - Leads to potential duplicate processing and also potential message loss\n",
    "\n",
    "\n",
    "- `auto.commit.interval.ms`: if offsets are committed automatically, this controls how frequently the offsets are committed in the poll loop\n",
    "\n",
    "\n",
    "### Explicitly Committing Offsets in Consumers\n",
    "\n",
    "- Needed when duplicates need to be minimized or event processing are done outside the main consumer poll loop\n",
    "\n",
    "#### Rules of Thumb\n",
    "\n",
    "- Always commit offsets after events were processed\n",
    "\n",
    "\n",
    "- Commit frequency is a trade-off between performance and number of duplicates in the event of a crash\n",
    "\n",
    "\n",
    "- Make sure to commit the exact correct offset\n",
    "\n",
    "\n",
    "- Rebalances will happen and they need to be handled properly\n",
    "\n",
    "\n",
    "- Consumers may need to retry if some records are not fully processed and will need to be processed later\n",
    "    - The consumer should not commit the offset of a message that it has not fully processed, the two common patterns:\n",
    "        - Commit the last record that was processed sucessfully, store the records that needs to be processed in a buffer and keep retrying, while keep polling to prevent a rebalance (consumer `pause()` method also makes additional polls not returning additional data to make retrying easier)\n",
    "        - Write the retriable record to a separate topic and continue, and use a separate consumer group to handle retries from the retry topic, or a consumer can subscribe to both the main and the retry topic but pause the retry topic between retries (similar to the dead-letter-queue system)\n",
    "        \n",
    "        \n",
    "- Consumers may need to maintain state\n",
    "    - Optionally write the latest state to a \"result\" topic at the same time the offset is committed\n",
    "    - It is recommended to use libraries like Kafka Streams, which provides high level DSL-like APIs for aggregations, joins, windows, and other complex analytics\n",
    "\n",
    "\n",
    "- Handling long processing times\n",
    "    - Keep polling (sending heartbeats to the broker) to stop a rebalance from happening\n",
    "    - Commonly a thread poll is used to parallelize processing of the records\n",
    "    - After handling off the records to the worker threads, you can pause the consumer and keep polling without actually fetching additional data until the worker threads finish\n",
    "\n",
    "\n",
    "- Exactly-once delivery\n",
    "    - *Idempotent Writes*: Commonly to do exactly-once, the results are written to a system that has support for unique keys, including all key-value stores, relational database, Elasticsearch, etc\n",
    "        - The common key is either from the record, or the combination of the topic, partition, and offset\n",
    "    - *Transactional Writes*: Write the result to a system that has transactions, such as a relational database; the idea is to write the records and their offsets in the same transaction so they will be in-sync\n",
    "        - When starting up, retrieve the offsets of the latest records written to the external store and then use `consumer.seek()` to start consuming again from those offsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating System Reliability\n",
    "\n",
    "### Validating Configuration\n",
    "\n",
    "- The broker and client configuration can be tested easily from the application logic\n",
    "    - Helps to test if the configuration chosen can meet the requirements\n",
    "    - It is good exercise to reason through the expected behavior of the system\n",
    "\n",
    "\n",
    "- `org.apache.kafka.tools` includes `VerifiableProducer` and `VerifiableConsumer` classes that can be run as command-line tools or be embedded in automated testing framework\n",
    "    - The verifiable producer produces a sequence of messages containing number 1 to N; and it cane be configured with the same configs applied on your producer (acks, retries, rate of production); upon execution, the producer print success or error for each message sent to the broker based on the acks received\n",
    "    - The verifiable consumer consumes events and prints out the events it consumed in order, along with information regarding commits and rebalances\n",
    "    \n",
    "\n",
    "- Test cases to run for:\n",
    "    - Leader election: kill the leader and observe the behavior of the producers and consumers\n",
    "    - Controller election: how long does it take the system to resume after a restart of the controller\n",
    "    - Rolling restart: restart the brokers one by one and no messages should be lost\n",
    "    - Unclean leader election test: kill all the replicas for a partition one by one to make sure each goes out of sync and then start a broker that was out of sync\n",
    "\n",
    "\n",
    "### Validating Application\n",
    "\n",
    "- Checking custom error-handling code, offset commits, and rebalance listeners, etc\n",
    "- Testing depends on the type of the application, but in general it is good to test under failure conditions:\n",
    "    - Clients lose connectivity to the cluster\n",
    "    - Leader election\n",
    "    - Rolling restart of brokers\n",
    "    - Rolling restart of consumers\n",
    "    - Rolling restart of producers\n",
    "\n",
    "\n",
    "### Monitoring Reliability in Production\n",
    "\n",
    "- For the producers, the two metrics most important for reliability are error-rate and retry-rate per record (aggregated); also monitor the producer logs for errors that occur while sending events that are logged at WARN; be aware of logs that indicate the producer running out of retries\n",
    "- For the consumers, the most important metric is consumer lag; make sure that consumers do eventually catch up rather than fall farther and farther behind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "text/x-java",
   "file_extension": ".java",
   "mimetype": "",
   "name": "Java",
   "nbconverter_exporter": "",
   "version": "1.8.0_121"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
