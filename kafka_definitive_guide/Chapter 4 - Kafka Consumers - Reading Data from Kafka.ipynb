{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bf3fef-4bd1-4dde-b80c-e928798fcfb4",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add mvn org.apache.kafka kafka_2.12 2.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consumers and Consumer Groups\n",
    "\n",
    "- Suppose the application reads messages from a Kafka topic, run some validations against them, and write the results to another data store\n",
    "- When a single consumer can keep up with the speed of message production, then you would not risk losing data\n",
    "- *Consumer groups* help facilitate and scale the consumption from topics, allowing multiple consumers to read from the same topic, splitting the data between them\n",
    "- Kafka consumers are typically part of a consumer group, when multiple consumers are subscribed to a topic and belong to the same consumer group, each consumer in the group will receive messages from a different subset of the partitions in the topic\n",
    "\n",
    "<img src=\"img/Snip20200424_6.png\" width=80%/>\n",
    "\n",
    "- A partition can be assigned to at most one consumer, when the number of consumer exceeds the number of partitions, some consumers will be idle and get no messages\n",
    "\n",
    "- In addition to adding consumers in order to scale a single application, it is common to have multiple applications that need to read data from the same topic\n",
    "    - In which scenarios, we want each application to get all of the messages, rather than a subset\n",
    "    - To ensure all application gets all the messages in a topic, each application needs to have its own consumer group\n",
    "    \n",
    "<img src=\"img/Snip20200424_7.png\" width=80%/>\n",
    "\n",
    "- In summary, create a new consumer group for each application that needs all the messages from one or more topics; add consumers to an existing consumer group to scale the reading and processing of messages from the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consumer Groups and Partition Rebalance\n",
    "\n",
    "- When a new consumer is added to the group, it starts consuming messages from partitions previously consumed by another consumer; same thing happens when a consumer shuts down or crashes\n",
    "- Reassignment of partitions to consumers also happen when the topics the consumer group is consuming are modified \n",
    "- Moving partition ownership from one consumer to another is called a *rebalance*, which is important since it provide the consumer group with high availability and scalability\n",
    "- During a rebalance, consumers can't consume messages, so a rebalance is basically a short window of unavailability of the entire consumer group\n",
    "    - Also, when partitions are moved from one consumer to another, the consumer loses its current state\n",
    "\n",
    "\n",
    "\n",
    "- Consumers maintain membership in a consumer group and ownership of the partitions assigned to them by sending *heartbeats* to a Kafka broker designated as the *group coordinator*\n",
    "    - If the consumer stops sending heartbeats for long enough, its session will time out and the group coordinator will consider it dead and trigger a rebalance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Kafka Consumer\n",
    "\n",
    "- First step to start consuming records is to create a `KafkaConsumer` instance\n",
    "- The three mandatory properties: `bootstrap.servers`, `key.deserializer`, and `value.deserializer`\n",
    "- Additionally, `group.id` specifies the consumer group the `KafkaConsumer` instance belongs to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package kafka_definitive_guide.consumer;\n",
    "\n",
    "import java.util.Properties;\n",
    "import org.apache.kafka.clients.consumer.ConsumerConfig;\n",
    "import org.apache.kafka.clients.consumer.KafkaConsumer;\n",
    "import org.apache.kafka.common.serialization.StringDeserializer;\n",
    "\n",
    "Properties kafkaProps = new Properties();\n",
    "kafkaProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"kafka:19092\");\n",
    "kafkaProps.put(ConsumerConfig.GROUP_ID_CONFIG, \"CountryCounter\");\n",
    "kafkaProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n",
    "kafkaProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n",
    "\n",
    "KafkaConsumer<String, String> consumer = new KafkaConsumer<>(kafkaProps);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offset and Consumer Position\n",
    "\n",
    "- Kafka maintains a numerical offset for each record in a partition\n",
    "- The offset acts as a unique identifier of a record within the partition and also denotes the position of the consumer in the partition\n",
    "    - E.g., a consumer at position 5 has consumed records with offsets 0 through 4\n",
    "\n",
    "\n",
    "- The `position` of the consumer gives the offset of the next record that will be given out, which is one larger than the highest offset the consumer has seen in the partition.\n",
    "    - This automatically advances every time the consumer receives messages in a call to `poll(Duration)`\n",
    "    \n",
    "    \n",
    "- The `committed position` is the last offset that has been stored securely. Should the process fail and restart, this is the offset that the consumer will recover to.\n",
    "    - The consumer can either automatically commit offsets periodically, or choose to control this committed position manually by calling one of the commit APIs (e.g., `commitSync` and `commitAsync`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subscribing to Topics\n",
    "\n",
    "- The consumer can subscribe to one or more topics\n",
    "- The `subscribe()` method takes a list of topics as a parameter\n",
    "\n",
    "```java\n",
    "consumer.subscribe(Collections.singletonList(\"customerCountries\"));\n",
    "```\n",
    "\n",
    "- It is also possible to call `subscribe` with a regular expression, which can match multiple topic names; and if a new topic is created with a name that matches, a rebalance will happen almost immediately and the consumers will start consuming from the new topic\n",
    "\n",
    "```java\n",
    "consumer.subscribe(\"my.topic.*\");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Poll Loop\n",
    "\n",
    "- The core of the consumer API is a loop for polling the server for more data\n",
    "- Once the consumer subscribes to topics, the poll loop handles all details of coordination, partition rebalances, heartbeats, and data fetching\n",
    "- The first time `poll` is called with a new consumer, it is responsible for finding the `GroupCoordinator`, joining the consumer group, and receiving a partition assignment\n",
    "- If a rebalance is triggered, it will be handled inside the poll loop\n",
    "- The heartbeats that keep consumers alive are sent from within the poll loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": " org.apache.kafka.common.errors.InterruptException",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mERROR: org.apache.kafka.common.errors.InterruptException: java.lang.InterruptedException\u001b[0;0m"
     ]
    }
   ],
   "source": [
    "package kafka_definitive_guide.consumer;\n",
    "\n",
    "import java.util.Properties;\n",
    "import java.time.Duration;\n",
    "import java.util.Collections;\n",
    "import org.apache.kafka.common.serialization.StringDeserializer;\n",
    "import org.apache.kafka.clients.consumer.ConsumerConfig;\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecords;\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecord;\n",
    "import org.apache.kafka.clients.consumer.KafkaConsumer;\n",
    "\n",
    "Properties kafkaProps = new Properties();\n",
    "kafkaProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"kafka:19092\");\n",
    "kafkaProps.put(ConsumerConfig.GROUP_ID_CONFIG, \"CountryCounter\");\n",
    "kafkaProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n",
    "kafkaProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n",
    "KafkaConsumer<String, String> consumer = new KafkaConsumer<>(kafkaProps);\n",
    "\n",
    "consumer.subscribe(Collections.singletonList(\"CustomerCountry\"));\n",
    "\n",
    "Duration pollingTimeout = Duration.ofMillis(100);\n",
    "\n",
    "try {\n",
    "    // Consumers are usually long-running applications that continuously\n",
    "    // poll Kafka for more data\n",
    "    while (true) {\n",
    "        // Consumers must keep polling Kafka or they will be considered dead\n",
    "        // and the partitions they are consuming will be handed to another consumer\n",
    "        // in the group to continue consuming\n",
    "        ConsumerRecords<String, String> records = consumer.poll(pollingTimeout);\n",
    "        // poll returns a list of records, each record contains the topic and \n",
    "        // partition the record came from, the offset of the record within the partition\n",
    "        // and the key-value pair of the record\n",
    "        for (ConsumerRecord<String, String> record : records) {\n",
    "            System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value());\n",
    "        }\n",
    "    }\n",
    "} finally {\n",
    "    consumer.close();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Consumers\n",
    "\n",
    "- `fetch.min.bytes`: the minimum amount of data that a consumer wants to receive from the broker when fetching records\n",
    "    - Used to reduce the load and the number of round trips between the consumer and the broker\n",
    "    - Along with `fetch.max.wait.ms` which controls how long the broker wait before responding to the consumer\n",
    "\n",
    "\n",
    "- `max.partition.fetch.bytes`: the maxium number of bytes the server will return per partition\n",
    "    - when `KafkaConsumer.poll()` returns `ConsumerRecords`, the record object will use at most `max.partition.fetch.bytes` per partition assigned to the consumer\n",
    "    - must be larger than `max.message.size`\n",
    "    - the consumer must call `poll()` frequently enough to avoid a session timeout, so if this is too large, it might take too long to process the messages and then do another poll\n",
    "\n",
    "\n",
    "- `session.timeout.ms`: the amount of time a consumer can be out of contact with the brokers while still considered alive\n",
    "     - if more than `session.timeout.ms` passes without the consumer sending a heartbeat to the group coordinator, it is considered dead and the group coordinator will trigger a rebalance of the consumer group to allocate partitions from the dead consumer to other consumers\n",
    "     - also `heartbeat.interval.ms` which controls how frequently `KafkaConsumer.poll()` sends a heartbeat to the group coordinator, usually `heartbeat.interval.ms` is one-third of `session.timeout.ms`\n",
    "\n",
    "\n",
    "- `auto.offset.reset`: controls the behavior of the consumer when it starts reading a partition for which it doesn't have a committed offset or if the committed offset it has is invalid\n",
    "    - default to *latest*, which means that lacking a valid offset, the consumer will start reading from the newest records\n",
    "    - or \"earliest\", which means the consumer will read all the data in the partition start from the very beginning\n",
    "\n",
    "\n",
    "- `enable.auto.commit`: controls whether the consumer will commit offsets automatically\n",
    "    - default to true, set to false if you prefer to control when offsets are committed\n",
    "    - also `auto.commit.interval.ms` controls how frequently offsets will be committed\n",
    "\n",
    "\n",
    "- `partition.assignment.strategy`: given consumers and topics they subscribed to, decides which partitions will be assigned to which consumer\n",
    "    - `RangeAssignor`: assign to each consumer a consecutive subset of partitions from each topic it subscribes to\n",
    "    - `RoundRobinAssignor`: takes all partitions from all subscribed topics and assigns them to consumers sequentially, one by one\n",
    "\n",
    "\n",
    "- `client.id`: used by the brokers to identify messages sent from the client, used in logging, metrics, and quotas\n",
    "\n",
    "\n",
    "- `max.poll.records`: controls the maximum number of records that a single call to `poll` will return\n",
    "\n",
    "\n",
    "- `receive.buffer.bytes` & `send.buffer.bytes`: size of the TCP send and receive buffers used by the sockets when writing and reading data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commits and Offsets\n",
    "\n",
    "- Kafka does not track acknowledgements from consumers, instead it allows consumers to use Kafka to track their position (offset) in each partition\n",
    "- The action of updating the current position in the partition is called a _commit_\n",
    "- The consumer commit an offset by producing a message to Kafka to a special `__consumer_offsets` topic with the committed offset for each partition\n",
    "- If a consumer crashes or a new consumer joins the consumer group, a rebalance will be triggered, after that, each consumer may be assigned a new set of partitions than the one it processed before; in order to know where to pick up the work, the consumer will read the latest committed offset of each partition and continue from there\n",
    "\n",
    "\n",
    "- Depending on the state of the committed offset and the last processed messages, it is possible that a consumer can re-process messages or miss messages\n",
    "\n",
    "<img src=\"img/Snip20200425_10.png\"/>\n",
    "<img src=\"img/Snip20200425_11.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Commit\n",
    "\n",
    "- By default, `enable.auto.commit` is set to true and every `auto.commit.interval.ms` (default to 5 seconds) the consumer will commit the largest offset the client received from `poll`\n",
    "- This commit is driven by the poll loop, whenever poll happens, the consumer checks if it is time to commit, and if it is, it commit the offsets it returned from the last poll\n",
    "- This option cannot completely eliminate duplicated records, due to the inevitable `auto.commit.interval.ms` old commit \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit Current Offset\n",
    "\n",
    "- To eliminate the possibility of missing messages and to reduce the number of messages duplicated during rebalancing, the consumer API has the option of committing the current offset at some point rather than based on a timer\n",
    "- By setting `enable.auto.commit` to false, offsets will only be committed when the application explicitly chooses to do so\n",
    "\n",
    "\n",
    "- `commitSync()`: commit the latest offset returned by `poll()` and return once the offset is committed, throwing an exception if commit fails for some reason\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package kafka_definitive_guide.consumer;\n",
    "\n",
    "import java.time.Duration;\n",
    "import java.util.Collections;\n",
    "import kafka_definitive_guide.consumer.ConsumerFactory;\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecords;\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecord;\n",
    "import org.apache.kafka.clients.consumer.KafkaConsumer;\n",
    "import org.apache.kafka.clients.consumer.CommitFailedException;\n",
    "\n",
    "Properties kafkaProps = new Properties();\n",
    "kafkaProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"kafka:19092\");\n",
    "kafkaProps.put(ConsumerConfig.GROUP_ID_CONFIG, \"CountryCounter\");\n",
    "kafkaProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\n",
    "kafkaProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n",
    "kafkaProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n",
    "KafkaConsumer<String, String> consumer = new KafkaConsumer<>(kafkaProps);\n",
    "\n",
    "consumer.subscribe(Collections.singletonList(\"CustomerCountry\"));\n",
    "\n",
    "\n",
    "try {\n",
    "    while (true) {\n",
    "        ConsumerRecords<String, String> records = consumer.poll(pollingTimeout);\n",
    "        for (ConsumerRecord<String, String> record : records) {\n",
    "            System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value());\n",
    "        }\n",
    "        try {\n",
    "            consumer.commitSync();\n",
    "        } catch (CommitFailedException e) {\n",
    "            // necessary error handling\n",
    "        }\n",
    "    }\n",
    "} finally {\n",
    "    consumer.close();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Commit\n",
    "\n",
    "- Synchronous commit blocks he application until the broker responds to the commit request, which limit the throughput of the application; throughput can be improved by committing less frequently, but the number of potential duplicates go up\n",
    "\n",
    "\n",
    "- `commitAsync`: send the commit request in an non-blocking way\n",
    "    - `commitAsync` will not retry unlike `commitSync`, because by the time `commitAsync` receives a response from the broker, there may have been a later commit that was already successful\n",
    "    - `commitAsync` also gives you an option  to pass in a callback that will be triggered when the broker responds; so commonly the callback is used to log commit errors or to count them in a metric\n",
    "    - optionally, one can retry in the commit callback, with the help of a monotonically increasing sequence number: increase the sequence number every time you commit and add the sequence number at the time of commit, upon retry, check if the commit sequence number the callback got is equal to the instance variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try {\n",
    "    while (true) {\n",
    "        ConsumerRecords<String, String> records = consumer.poll(pollingTimeout);\n",
    "        for (ConsumerRecord<String, String> record : records) {\n",
    "            System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value());\n",
    "        }\n",
    "        consumer.commitAsync(new OffsetCommitCallback() {\n",
    "            public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets,\n",
    "                                   Exception e) {\n",
    "                if (e != null) log.error(\"Commit failed for offsets {}\", offsets, e);\n",
    "            }\n",
    "        });\n",
    "    }\n",
    "} finally {\n",
    "    consumer.close();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Synchronous and Asynchronous Commits\n",
    "\n",
    "- Occasional failures to commit without retrying are usually ignorable because the errors can be transient\n",
    "- If we know that this is the last commit before we close the consumer, or before a rebalance, we want to make sure that the commit succeeds\n",
    "- A common pattern is to combine `commitAsync` with `commitSync` just before shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try {\n",
    "    while (true) {\n",
    "        ConsumerRecords<String, String> records = consumer.poll(pollingTimeout);\n",
    "        for (ConsumerRecord<String, String> record : records) {\n",
    "            System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value());\n",
    "        }\n",
    "        consumer.commitAsync();\n",
    "    }\n",
    "} finally {\n",
    "    try {\n",
    "        consumer.commitSync();\n",
    "    } finally {\n",
    "        consumer.close();\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit Specified Offset\n",
    "\n",
    "- `commitSync()` and `commitAsync()` optionally accepts a map of partitions and offsets that need to be committed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();\n",
    "int count = 0;\n",
    "\n",
    "while (true) {\n",
    "    ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "    for (ConsumerRecord<String, String> record : records) {\n",
    "        System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value());\n",
    "        currentOffsets.put(new TopicPartition(record.topic(), record.partition()),\n",
    "                           new OffsetAndMetadata(record.offset() + 1, \"no meta\"));\n",
    "        if (count++ % 1000 == 0) {\n",
    "            consumer.commitAsync(currentOffests, null);\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebalance Listeners\n",
    "\n",
    "- A consumer will want to do some cleanup work before exiting and also before partition rebalancing; if the consumer is about to lose ownership of a partition, you will want to commit offsets of the last event you've processed\n",
    "\n",
    "\n",
    "- The consumer API allows you to run code when partitions are added or remove from the consumer\n",
    "- `subscribe()` takes a `ConsumerRebalanceListener` as parameter\n",
    "    - `public void onPartitionsRevoked(Collection<TopicPartition> partitions)`: called before the rebalancing starts and after the consumer stopped consuming messages; where the consumer should commit offsets\n",
    "    - `public void onPartitionsAssigned(Collection<TopicPartition> partitions)`: called after partitions have been reassigned to the broker, but before the consumer starts consuming messages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package kafka_definitive_guide.consumer;\n",
    "\n",
    "import java.util.*;\n",
    "import org.apache.kafka.common.*;\n",
    "import org.apache.kafka.clients.consumer.*;\n",
    "\n",
    "Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();\n",
    "\n",
    "class HandleRebalance implements ConsumerRebalanceListener {\n",
    "    private Consumer<?, ?> consumer;\n",
    "    \n",
    "    public HandleRebalance(Consumer<?, ?> consumer) {\n",
    "        this.consumer = consumer;\n",
    "    }\n",
    "    \n",
    "    @Override\n",
    "    public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n",
    "        // no-op\n",
    "    }\n",
    "    \n",
    "    @Override\n",
    "    public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n",
    "        consumer.commitSync(currentOffsets);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consuming Records with Specific Offsets\n",
    "\n",
    "- `seekToBeginning(TopicPartition tp)`: read all messages from the beginning of the partition\n",
    "- `seekToEnd(TopicPartition tp):` skip all the way to the end of the partition and start consuming only new messages\n",
    "- `seek()`: starts reading from a specific offset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public class SaveOffsetsOnRebalance implements ConsumerRebalanceListener {\n",
    "    public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n",
    "        for (TopicPartition partition: partitions) {\n",
    "            consumer.seek(partition, getOffsetFromDB(partition));\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n",
    "        commitDBTransaction();\n",
    "    }\n",
    "}\n",
    "\n",
    "consumer.subscribe(topics, new SaveOffsetsOnRebalance(consumer));\n",
    "consumer.poll(0); // ensure we join a consumer group and get assigned partitions\n",
    "for (TopicPartition partition: consumer.assignment()) {\n",
    "    // then immediately seek to the correct offset in the partitions assigned\n",
    "    consumer.seek(partition, getOffsetFromDB(partition));\n",
    "}\n",
    "\n",
    "while (true) {\n",
    "    ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "    for (ConsumerRecord<String, String> record : records) {\n",
    "        processRecord(record);\n",
    "        saveRecordToDB(record);\n",
    "        saveOffsetToDB(record.topic(), record.partition(), record.offset());\n",
    "    }\n",
    "    commitDBTransaction();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exiting from the Poll Loop\n",
    "\n",
    "- Call `consumer.wakeup()` from another thread, which will cause `poll()` to exit with `WakeupException`, which does not need to be handled, but before exiting the thread, `consumer.close()` must be invoked\n",
    "\n",
    "- If the consumer loop is running in the main thread, this can be done from `ShutdownHook` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Runtime.getRuntime().addShutdownHook(new Thread() {\n",
    "    public void run() {\n",
    "        // shutdown hook runs in a separate thread\n",
    "        consumer.wakeup();\n",
    "        try {\n",
    "            mainThread.join();\n",
    "        } catch (InterruptedException e) {\n",
    "            e.printStackTrace();\n",
    "        }\n",
    "    }\n",
    "});\n",
    "    \n",
    "try {\n",
    "    while (true) {\n",
    "        ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "        for (ConsumerRecord<String, String> record : records) {\n",
    "            System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value());\n",
    "        }\n",
    "        consumer.commitSync();\n",
    "    }\n",
    "} catch (WakeupException e) {\n",
    "    // no-op\n",
    "} finally {\n",
    "    consumer.close();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deserializers\n",
    "\n",
    "- Kafka consumers require *deserializers* to convert byte arrays from Kafka into Java objects\n",
    "- Serializers in producers and deserializers in consumers must match; one of the benefits of using Avro and the Schema Repository for serializing and deserializing is that the `AvroSerializer` can ensure that all the data written to a specific topic is compatible with the schema of the topic, which means it can be deserialized with the matching deserializer and schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standalone Consumer without a Consumer Group\n",
    "\n",
    "- Occasionally you only need a single consumer that always needs to read data from all the partitions in a topic, or from a specific partition in a topic; in which case there is no reason for groups or rebalances; just **assign** the consumer-specific topic and/or partitions, consume messages, and commit offsets\n",
    "\n",
    "\n",
    "- A consumer can either subscribe to topics and be part of a consumer group, or assign itself partitions, but not both at the same time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "cannot find symbol",
     "evalue": "cannot find symbol",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mcannot find symbol\u001b[0;0m",
      "\u001b[1;31m  symbol:   class List\u001b[0;0m",
      "\u001b[0;31m  location: class com.twosigma.beaker.javash.bkr3ff7b631.BeakerWrapperClass1261714175Id3600b9a2c80840e28b13564f19cf09f3\u001b[0;0m",
      "\u001b[0;31m List<PartitionInfo> partitionInfos\u001b[0;0m",
      "\u001b[0;31m ^   ^                               \u001b[0;0m",
      "\u001b[0;31m\u001b[0;0m",
      "\u001b[0;31mcannot find symbol\u001b[0;0m",
      "\u001b[0;31m  symbol:   class PartitionInfo\u001b[0;0m",
      "\u001b[0;31m  location: class com.twosigma.beaker.javash.bkr3ff7b631.BeakerWrapperClass1261714175Id3600b9a2c80840e28b13564f19cf09f3\u001b[0;0m",
      "\u001b[0;31m List<PartitionInfo> partitionInfos = consumer.pa\u001b[0;0m",
      "\u001b[0;31m      ^            ^                               \u001b[0;0m",
      "\u001b[0;31m\u001b[0;0m",
      "\u001b[0;31mcannot find symbol\u001b[0;0m",
      "\u001b[0;31m  symbol:   variable consumer\u001b[0;0m",
      "\u001b[0;31m  location: class com.twosigma.beaker.javash.bkr3ff7b631.BeakerWrapperClass1261714175Id3600b9a2c80840e28b13564f19cf09f3\u001b[0;0m",
      "\u001b[0;31martitionInfo> partitionInfos = consumer.partitionsFor(\"topic\")\u001b[0;0m",
      "\u001b[0;31m                               ^       ^                        \u001b[0;0m",
      "\u001b[0;31m\u001b[0;0m",
      "\u001b[0;31mcannot find symbol\u001b[0;0m",
      "\u001b[0;31m  symbol:   class List\u001b[0;0m",
      "\u001b[0;31m  location: class com.twosigma.beaker.javash.bkr3ff7b631.BeakerWrapperClass1261714175Id3600b9a2c80840e28b13564f19cf09f3\u001b[0;0m",
      "\u001b[0;31m List<TopicPartition> partitions = \u001b[0;0m",
      "\u001b[0;31m ^   ^                               \u001b[0;0m",
      "\u001b[0;31m\u001b[0;0m",
      "\u001b[0;31mcannot find symbol\u001b[0;0m",
      "\u001b[0;31m  symbol:   class TopicPartition\u001b[0;0m",
      "\u001b[0;31m  location: class com.twosigma.beaker.javash.bkr3ff7b631.BeakerWrapperClass1261714175Id3600b9a2c80840e28b13564f19cf09f3\u001b[0;0m",
      "\u001b[0;31m List<TopicPartition> partitions = new ArrayList<>\u001b[0;0m",
      "\u001b[0;31m      ^             ^                               \u001b[0;0m",
      "\u001b[0;31m\u001b[0;0m",
      "\u001b[0;31mcannot find symbol\u001b[0;0m",
      "\u001b[0;31m  symbol:   class ArrayList\u001b[0;0m",
      "\u001b[0;31m  location: class com.twosigma.beaker.javash.bkr3ff7b631.BeakerWrapperClass1261714175Id3600b9a2c80840e28b13564f19cf09f3\u001b[0;0m",
      "\u001b[0;31mpicPartition> partitions = new ArrayList<>()\u001b[0;0m",
      "\u001b[0;31m                               ^        ^     \u001b[0;0m",
      "\u001b[0;31m\u001b[0;0m",
      "\u001b[0;31mcannot find symbol\u001b[0;0m",
      "\u001b[0;31m  symbol:   class PartitionInfo\u001b[0;0m",
      "\u001b[0;31m  location: class com.twosigma.beaker.javash.bkr3ff7b631.BeakerWrapperClass1261714175Id3600b9a2c80840e28b13564f19cf09f3\u001b[0;0m",
      "\u001b[0;31m for (PartitionInfo partition: partitionInfos) {\u001b[0;0m",
      "\u001b[0;31m      ^            ^                              \u001b[0;0m",
      "\u001b[0;31m\u001b[0;0m",
      "\u001b[0;31mcannot find symbol\u001b[0;0m",
      "\u001b[0;31m  symbol:   class TopicPartition\u001b[0;0m",
      "\u001b[0;31m  location: class com.twosigma.beaker.javash.bkr3ff7b631.BeakerWrapperClass1261714175Id3600b9a2c80840e28b13564f19cf09f3\u001b[0;0m",
      "\u001b[0;31m partitions.add(new TopicPartition(partition.topic(), partition.\u001b[0;0m",
      "\u001b[0;31m                    ^             ^                               \u001b[0;0m",
      "\u001b[0;31m\u001b[0;0m",
      "\u001b[0;31mcannot find symbol\u001b[0;0m",
      "\u001b[0;31m  symbol:   variable consumer\u001b[0;0m",
      "\u001b[0;31m  location: class com.twosigma.beaker.javash.bkr3ff7b631.BeakerWrapperClass1261714175Id3600b9a2c80840e28b13564f19cf09f3\u001b[0;0m",
      "\u001b[0;31m consumer.assign(partitions)\u001b[0;0m",
      "\u001b[0;31m ^       ^                    \u001b[0;0m"
     ]
    }
   ],
   "source": [
    "List<PartitionInfo> partitionInfos = consumer.partitionsFor(\"topic\");\n",
    "List<TopicPartition> partitions = new ArrayList<>();\n",
    "\n",
    "if (partitionInfos != null) {\n",
    "    for (PartitionInfo partition: partitionInfos) {\n",
    "        partitions.add(new TopicPartition(partition.topic(), partition.partition()));\n",
    "    }\n",
    "    consumer.assign(partitions);\n",
    "    \n",
    "    while (true) {\n",
    "        ConsumerRecords<String, String> records = consumer.poll(pollingTimeout);\n",
    "        for (ConsumerRecord<String, String> record : records) {\n",
    "            System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value());\n",
    "        }\n",
    "        consumer.commitSync();\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "text/x-java",
   "file_extension": ".java",
   "mimetype": "",
   "name": "Java",
   "nbconverter_exporter": "",
   "version": "1.8.0_121"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
