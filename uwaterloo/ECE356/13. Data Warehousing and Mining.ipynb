{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Support Systems\n",
    "\n",
    "- **Decision-support systems** are used to make business decisions, often based on data collected using OLTP systems\n",
    "- **Data analysis** tasks are simplified by specialized tools and SQL extensions \n",
    "- **Data mining** seeks to discover knowledge automatically in the form of statistical rules and patterns from large databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Warehousing\n",
    "\n",
    "- data sources often store only current data, not historical data\n",
    "- corporate decision making requires a unified view of all organizational data, including historical data\n",
    "- a **data warehouse** archives information gathered from multiple sources, and stores it under a unified schema, at a single site\n",
    "    - important for large businesses that generate data from multiple divisions, possibly at multiple sites\n",
    "    - simplifies querying and permits study of historical trends\n",
    "    - shifts decision support query load away from transaction processing systems and into other tools\n",
    "    \n",
    "<img src=\"img/Snip20191111_220.png\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Challenges\n",
    "\n",
    "- *when and how to gather data*\n",
    "    - **Source driven architecture**: data sources transmit new information to warehouse, either continuously or periodically\n",
    "    - **Destination driven architecture**: warehouse periodically requests new information from data sources\n",
    "    - keeping warehouse exactly synchronized with data sources is too expensive\n",
    "- *what schema to use*\n",
    "    - schema integration\n",
    "- *data cleansing*\n",
    "    - e.g., **correct** mistakes in attributes (e.g., addresses)\n",
    "    - or **merge** address lists from different sources and **purge** duplicates\n",
    "- *how to propagate updates*\n",
    "    - warehouse schema may be a (materialized) view of schema from data sources\n",
    "- *what data to summarize*\n",
    "    - raw data may be too large to store \n",
    "    - aggregate values (totals/subtotals) often suffice\n",
    "    - queries on raw data canoften be transformed by query optimizer to use aggregate values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warehouse Schemas\n",
    "\n",
    "- warehouses generally organize data into **fact tables** and **dimension tables**\n",
    "\n",
    "\n",
    "- **Fact tables** describe specific events (e.g., transactions), and contain mostly *small numeric values* as well as *foreign keys* pointing to rows of dimension tables\n",
    "    - fact tables can be very large\n",
    "\n",
    "\n",
    "- **Dimension tables** store descriptive data, for example referring to a time, a place, a product, or a person\n",
    "    - tend to record a larger number of attributes, including both numeric and text\n",
    "    - tend to contain fewer rows than fact tables\n",
    "    \n",
    "\n",
    "- resultant schema is called a **star schema**, more elaborate schema structures are possible\n",
    "    - **Snowflake schema**: multiple levels of dimension tables\n",
    "    - **Constellation**: multiple fact tables\n",
    "\n",
    "- Example of Star Schema\n",
    "\n",
    "<img src=\"img/Snip20191111_221.png\" width=80%/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cubes\n",
    "\n",
    "- a star schema represents multidimensional data, which is difficult to visualize\n",
    "- OLAP systems enable data summarization and interactive exploration of a multidimensional data set using the **data cube** abstraction\n",
    "- each cell of the data cube holds a data item corresponding to an intersection of dimensions\n",
    "- a **slicer** is a dimension that is held constant so that the cube can be collapsed onto fewer dimensions\n",
    "\n",
    "<img src=\"img/Snip20191111_222.png\" width=60%/>\n",
    "\n",
    "## Slicing\n",
    "\n",
    "<img src=\"img/Snip20191111_223.png\" width=80%/>\n",
    "\n",
    "## Dicing \n",
    "\n",
    "<img src=\"img/Snip20191111_224.png\" width=80%/>\n",
    "\n",
    "## Drill-up/Drill-down\n",
    "\n",
    "<img src=\"img/Snip20191111_225.png\" width=80%/>\n",
    "\n",
    "## Pivoting\n",
    "\n",
    "<img src=\"img/Snip20191111_226.png\" width=80%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining\n",
    "\n",
    "- the process of **semi-automatically** analyzing large databases to find **useful patterns**\n",
    "\n",
    "\n",
    "- data mining is often used for **prediction**\n",
    "    - e.g., predict if a credit card applicant poses a good credit risk\n",
    "- examples of prediction mechanisms\n",
    "    - **classification**: given a new item whose class is unknown, predict to which class it belongs\n",
    "        - e.g., classify a blog post as either positive or negative\n",
    "    - **regression**: given a set of mappings for an unknown function, predict the function result for a new parameter value\n",
    "        - e.g., given the outdoor temperature measurements for the last week, predict tomorrow's outdoor temperature\n",
    "\n",
    "\n",
    "- other applications of data mining aim to **identify descriptive patterns** in existing data\n",
    "- examples of descriptive patterns\n",
    "    - **associations**: \"if-then\" patterns\n",
    "        - e.g., if a customer C purchases book b, then they are likely to enjoy book B' because other customers \"similar\" to C have purchased both B and B'\n",
    "    - **clustering**: discover groups of similar objects\n",
    "        - e.g., mobile network users are clustered in certain areas, suggesting where cellular towers should be placed\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining - Classification\n",
    "\n",
    "## Classification Rules\n",
    "\n",
    "- classification rules help assign new objects to classes\n",
    "- classification rules can use a variety of data\n",
    "- rules are not necessarily exact: there may be some misclassifications\n",
    "- classification rules can be represented compactly as a decision tree\n",
    "\n",
    "<img src=\"img/Snip20191111_227.png\" width=80%/>\n",
    "\n",
    "## Construction of Decision Trees\n",
    "- **training set**: a sample of instances for which the classification is already known\n",
    "- the decision tree is generated from the training set using a **greedy** top-down approach\n",
    "    - each internal node of the tree partitions the data into groups based on a **partitioning attribute**, and a **partitioning condition** for the node\n",
    "    - at each **leaf** node, either all (or most) of the items at the node belong to the same calss, or else all attributes have been considered and no further partitioning is possible\n",
    "\n",
    "## Best Splits\n",
    "\n",
    "- a traversal of the decision tree begins with \"impure\" data (instances from many classes) at the root and terminates with \"pure\" data (instances from one class only) at the leaf level\n",
    "\n",
    "- the main **goal in building a decision tree** is to pick the best attributes and conditions on which to partition at each level so as the reduce the \"impurity\"\n",
    "\n",
    "- several quantitative measures of **impurity** have been proposed over a set $S$ of **training instances**\n",
    "\n",
    "- $k$: number of classes\n",
    "- $|S|$: number of instances\n",
    "- $p_i$: fraction of instances in class $i$\n",
    "\n",
    "## Impurity Measures: Gini\n",
    "\n",
    "- the **Gini** measure of impurity is defined as\n",
    "\\begin{equation}\n",
    "\\textrm{Gini}(S) = 1 - \\sum_{i=1}^k p_i^2\n",
    "\\end{equation}\n",
    "\n",
    "- if all instances are in a single class (i.e., maximum purity), the $\\textrm{Gini}$ value is 0\n",
    "- if each class has the same number of instances (i.e., minimum purity), the value if $1 - {1\\over k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity Measures: Entropy\n",
    "\n",
    "- another measure of impurity is the **entropy** measure, which is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{Entropy}(S) = -\\sum_{i=1}^k p_i \\log_2 p_i\n",
    "\\end{equation}\n",
    "\n",
    "- if all instances are in a single class (i.e., maximum purity), the entropy value is 0\n",
    "    - note: $p_i \\log_2 p_i$ is defined as 0 for $p_i = 0$\n",
    "- if each class has the same number of instances (i.e., minimum purity) the value is $-\\log_2 {1\\over k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "\n",
    "- when a set $S$ is split into multiple sets $S_i, i = 1, 2, ..., r$, we can measure the impurity of the resultant set of sets as\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{Impurity}(S_1, S_2,...,S_r) = \\sum_{i = 1}^r {|S_i| \\over |S|} \\textrm{Impurity}(S_i)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- the **information gain** due to a particular split of $S$ into $S_i, i = 1, 2, ..., r$ is defined as follows\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{Information-gain}(S, \\{S_1, S_2, ..., S_r\\}) = \\textrm{Impurity}(S) - \\textrm{Impurity}(S_1, S_2, ..., S_r)\n",
    "\\end{equation}\n",
    "\n",
    "- a good split always achieves a **positive** information gain\n",
    "\n",
    "\n",
    "- measure of \"cost\" of a split\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{Information-content}(S, \\{S_1, S_2, ..., S_r\\}) = -\\sum_{i=1}^r {|S_i| \\over |S|} \\log_2 {|S_i| \\over |S|}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- measure of \"goodness\" of a split\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{Information gain ratio} = {\\textrm{Information-gain(S, \\{S_1, S_2, ..., S_r\\})} \\over \\textrm{Information-content(S, \\{S_1, S_2, ..., S_r\\})}}\n",
    "\\end{equation}\n",
    "\n",
    "- the best split (i.e., one that tends to yield the simplest and most meaningful decision tree) is the one that produces the **maximum information gain ratio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Best Splits\n",
    "\n",
    "- **Categorical attributes** (with no meaningful order)\n",
    "    - binary split: try all possible ways to partition the values into two disjoint sets, and pick the best\n",
    "    - multi-way split: one child for each value\n",
    "- **Continuous-valued attributes** (with meaningful order)\n",
    "    - binary split: sort values, try each as a split point\n",
    "    - multi-way split: a series of binary splits on the same attribute has roughly equivalent effect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Construction\n",
    "\n",
    "<img src=\"img/Snip20191115_11.png\" width=60%/>\n",
    "\n",
    "- $\\delta_p$ and $\\delta_s$ are user-defined thresholds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Classifiers\n",
    "\n",
    "- *Bayesian classifiers* use **Bayes theorem**, which states\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "p(c_j | d) = {p(d | c_j) p(c_j) \\over p(d)}\n",
    "\\end{equation}\n",
    "\n",
    "- $p(c_j | d)$: probability of instance $d$ being in class $c_j$\n",
    "- $p(d|c_j)$: probability of generating instance $d$ given class $c_j$\n",
    "- $p(c_j)$: probability of occurrence of class $c_j$\n",
    "- $p(d)$: probability of occurrence  of instance $d$\n",
    "\n",
    "## Naive Bayesian Classifiers\n",
    "\n",
    "- Bayesian classifiers require\n",
    "    - computation of $p(d | c_j)$\n",
    "    - pre-computation of $p(c_j)$\n",
    "    - $p(d)$ can be ignored since it is the same for all classes\n",
    "\n",
    "\n",
    "- to simplify, **naive Bayesian classifiers** assume attributes have *independent distributionsâ€¢, and thereby estimate\n",
    "\\begin{equation}\n",
    "p(d|c_j) = p(d_1 | c_j) \\times p(d_2 | c_j)\\times ... \\times p(d_n | c_j)\n",
    "\\end{equation}\n",
    "    - each of the $p(d_i | c_j)$ can be estimated from a histogram on $d_i$ values for each class $c_j$\n",
    "    - histograms are computed from training instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating a Classifer\n",
    "\n",
    "- the quality of a classifier can be quantified along several dimensions\n",
    "\n",
    "- for a Boolean classifier, outcomes fall into 4 categories\n",
    "    - **True Positive**(TP): correct, prediction was positive, instance is positive\n",
    "    - **False Positive**(FP): incorrect, prediction was positive, instance is negative\n",
    "    - **True Negative**(TN): correct, prediction was negative, instance is negative\n",
    "    - **False Negative**(FN): incorrect, prediction was negative, instance is positive\n",
    "    \n",
    "    \n",
    "- the quality of a classifier can be described in several ways\n",
    "    - **accuracy**: fraction of instances where classifier is correct: $TP + TN \\over TP + FP + TN + FN$\n",
    "    - **recall**: fraction of positive instances correctly classified: $TP \\over TP + FN$ (aka *true positive rate*)\n",
    "    - **precision**: fraction of correct positive predictions: $TP \\over TP + FP$\n",
    "    - **specificity**: fraction of negative instances correctly classified: $TN \\over FP + TN$ (aka *true negative rate*)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "- a simple way to validate a classifier is to split a given data set (sample) into disjoint training and testing parts\n",
    "- both parts are labelled with the ground truth (i.e., the correct classification)\n",
    "    - required in the training part to compute the model, and in the testing part for validation\n",
    "- in **k-fold cross-validation**, a split is computed $k$ times, the data set is first divided randomly into $k$ parts (folds) of equal size\n",
    "    - each part is used to validate a model computed using the remaining $k-1$ parts\n",
    "- measures of quality are computed separately for each part, and then averaged\n",
    "    - the accuracy of the constructed model is the average of the $k$ accuracy figuerd computed for different parts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "- regression deals with the prediction of a value, rather than a class\n",
    "- given values for a set of variables $X_1, X_2,...,X_n$, we want to predict the value of a variable $Y$\n",
    "- **linear regression**: infer coeeficients $a_0, a_1, ..., a_n$ such that $Y = a_0 + a_1 X_1 + a_2 X_2 + ... + a_n X_n$\n",
    "- in general, the process of finding a curve that fits the data is also called **curve fitting**\n",
    "- regression aims to find coefficients that give the best possible fit (e.g., minimizes sum of squared residuals)\n",
    "- the fit may only be approximate because of noise in the data, or because the relationship does not follow exactly the type of curve being fitted (e.g., polynomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rule Mining\n",
    "\n",
    "- **association rules**\n",
    "    - $bread => milk$\n",
    "    - left side is the **antecedent**, right side is the **consequent**\n",
    "    - an association rule must have an associated **population**, the population consists of a set of **instances**\n",
    "        - e.g., each transaction at a shop is an instance, and the set of all transactions is the population\n",
    "\n",
    "\n",
    "- rules have an associated \"support\" and an associated \"confidence\"\n",
    "- **Support**: a measure of what fraction of the population satisfies both the antecedent and the consequent of a rule\n",
    "    - e.g., suppose only 0.0001% of all purchases include both milk and screwdrivers, then the degree of support for the rule $milk => screwdrivers$ is low\n",
    "    \n",
    "\n",
    "- **Confidence**: a measure of how often the consequent is true when the antecedent is true\n",
    "    - e.g., the rule $bread => milk$ has a confidence of 80% if 80% of the purchases that include bread also include milk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Association Rules\n",
    "\n",
    "- we are generally only interested in association rules with reasonably high (e.g., few %) support\n",
    "\n",
    "- naive algorithm\n",
    "    1. consider all possible sets of relevant items\n",
    "    2. compute the support for each set and identify sets with sufficiently high support (e.g., based on a threshold)\n",
    "    3. select **large itemsets** with sufficiently high support\n",
    "    4. use these large itemsets to generate association rules, from itemset $A$ and each $b \\in A$ generate the rule $A-\\{b\\} => b$\n",
    "        - support of rule: $support(A)$\n",
    "        - confidence: $support(A) \\over support(A - \\{b\\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori Algorithm \n",
    "\n",
    "- computes large itemsets given a set $T$ of transactions over a set of items, and a support threshold $\\mathcal{E}$\n",
    "\n",
    "<img src=\"img/Snip20191115_12.png\" width=80%/>\n",
    "\n",
    "### Apriori Example\n",
    "\n",
    "<img src=\"img/Snip20191115_14.png\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "- finding clusters of points in the given data such that similar points lie in the same cluster\n",
    "- **k-means**: group points into $k$ sets (for a given $k$) such that the average distance (e.g., Euclidean distance) of points from the centroid of their assigned set is minimized\n",
    "    - **centroid**: the mean of a sset of points\n",
    "\n",
    "## $k$-Means Clustering Algorithm\n",
    "\n",
    "- input: number of clusters ($k$) and set of input points $p_1, ..., p_n$\n",
    "- initialization: place centroids $c_1, ..., c_k$ randomly\n",
    "- loop\n",
    "    - for each point $x_i$, find the nearest (in terms of Euclidian distance) centroid $c_j$ and assign point $x_i$ to cluster $j$\n",
    "    - for each non-empty clutser $j$, let the new centroid $c_j$ be the mean of all points assigned to cluster $j$ in the previous step\n",
    "    - for each empty cluster $j$, re-initialize the centroid $c_j$ randomly\n",
    "    - exit if none of the centroids $c_1, ..., c_k$ has changed since the previous iteration (i.e., convergence has occurred)\n",
    "\n",
    "\n",
    "## Hierarchical Clustering\n",
    "\n",
    "- **agglomerative clustering algorithms**: build small clusters, then cluster small clusters into bigger clusters and so on\n",
    "- **divisive clustering algorithms**: start with all items in a single cluster, repeatedly refine (break) clusters into smaller ones (top-down)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
