{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Intensive Applications\n",
    "\n",
    "- data-intensive: CPU power is rarely a limiting factor, but rather the amount, the complexity, and the changing speed of the data\n",
    "- a data-intensive application is built from standard building blocks that provide commonly needed functionality\n",
    "    - store data so applications can find it later (databases)\n",
    "    - remember the result of an expensive operation to speed up reads (caches)\n",
    "    - allow users to search data by keyword or filter it in various ways (search indexes)\n",
    "    - send a message to another process to be handled asynchronously (stream processing)\n",
    "    - periodically crunch a large amount of accumulated data (batch processing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability \n",
    "\n",
    "- working correctly and continue to do so even when things go wrong\n",
    "\n",
    "### Hardware Faults\n",
    "\n",
    "- e.g., hard disks are reported as having a MTTF (mean time to failure) of about 10 to 50 years, thus on a storage cluster with 10,000 disks, we can expect on average one disk to die per day\n",
    "- first response is usually to add redundancy to the individual hardware\n",
    "- as data volumes and applications' computing demands increase, more applications have begun using larger numbers of machines, which proportionally increases the rate of hardware faults\n",
    "- there is a move toward systems that can tolerate the loss of entire machines, by using software fault-tolerance techniques\n",
    "\n",
    "\n",
    "### Software Errors\n",
    "- hardware faults are random and independent from each other (usually)\n",
    "- systematic error within the system: harder to anticipate, correlated across nodes, cause many more system failures \n",
    "    - software bugs\n",
    "    - a runaway process that occupies shared resource\n",
    "    - a service that the system depends on fails\n",
    "    - cascading failures, where a small fault in one component triggers a fault in another component, which in turn triggers further faults\n",
    "- potential solutions\n",
    "    - thorough testing\n",
    "    - process isolation\n",
    "    - allowing processes to crash and restart\n",
    "    - measuring, monitoring, and analyzing system behavior\n",
    "    - SLA monitoring\n",
    "\n",
    "### Human Errors\n",
    "- design systems in a way that minimizes opportunities for error\n",
    "    - well designed abstractions, APIs to encourage \"golden path\" and discourage sketchy operations\n",
    "    - decouple error-prone areas from failure-sensitive areas\n",
    "        - e.g., provide development sandbox \n",
    "    - test thoroughly\n",
    "    - allow quick and easy recovery from human errors, to minimize the impact\n",
    "        - make it easy and fast to rollback config changes\n",
    "        - rolling deploy\n",
    "    - detailed and clear monitoring, such as performance metrics and error rates\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability\n",
    "\n",
    "- the system's ability to cope with increased load\n",
    "- if a system grows in a particular way, what are the options for coping with the growth\n",
    "\n",
    "### Describing Load\n",
    "- load can be described with a few numbers called *load parameters*\n",
    "    - requests per second to a web server\n",
    "    - ratio of reads to writes in a database\n",
    "    - number of simultaneously active users in a chat room\n",
    "    - hit rate on a cache\n",
    "\n",
    "**example: twitter**\n",
    "- operations involved: post tweet and browse home timeline\n",
    "    - post tweet: publish a new message to followers (4.6k requests/sec on avg, over 12k requests/sec at peak)\n",
    "    - home timeline: view tweets posted by users they follow (300k requests/sec)\n",
    "- handling 12,000 writes per second would be easy, but twitter's scaling challenge is not primarily due to tweet volume, but due to *fan-out*\n",
    "    - *fan-out*: the output needs to supply enough current to drive all the attached inputs; in transaction processing systems, it refers to the number of requests to other services that we need to make in order to serve one incoming request\n",
    "- implementation 1: global collection\n",
    "    - posting simply inserts the new tweet into a global collection of tweets\n",
    "    - when user requests home timeline, look up users they follow and find all the tweets for each of those users and merge them\n",
    "\n",
    "```\n",
    "SELECT tweets.*, users.* FROM tweets\n",
    "JOIN users ON tweets.sender_id = users.id\n",
    "JOIN follows ON follows.followee_id = users.id\n",
    "WHERE follows.follower_id = current_user\n",
    "```\n",
    "\n",
    "<img src=\"img/Snip20190913_61.png\"/>\n",
    "\n",
    "- implementation 2: mailbox\n",
    "    - maintain a cache for each user's home timeline\n",
    "    - when a user posts a tweet, look up all the people who follow the user, and insert the new tweet into each of their home timeline caches\n",
    "    - read home timeline is then cheap because the result is computed ahead of time\n",
    "    \n",
    "<img src=\"img/Snip20190913_62.png\"/>\n",
    "\n",
    "- the global collection approach struggled to keep up with the load of home timeline queries\n",
    "- the mailbox approach works better because the average rate of published tweets is almost two orders of magnitude lower than the rate of home timeline reads, so in this case it's preferable to do more work at write time and less at read time\n",
    "- the mailbox approach is extremely expensive when the tweet poster has many followers\n",
    "- so the distribution of followers per user (maybe weighted by how often those users tweet) is a **key load parameter** for scalability as it determines the fan-out load\n",
    "\n",
    "### Describing Performance\n",
    "\n",
    "- investigate what happens when the load increases\n",
    "    - what is the performance impact when a certain load parameter is increased if system resources remain unchanged\n",
    "    - how much system resources need to be scaled up if a load parameter is increased, to keep performance unchanged\n",
    "- in batch processing, *throughput* \n",
    "- in online systems, *response time*\n",
    "- *percentiles*: sort a list of metrics from low to high\n",
    "    - *p50*, *p95*, *p99*, *p999*\n",
    "    - if the 95th percentile response time is 1.5 seconds, then 95 out of 100 requests take less than 1.5 seconds\n",
    "    - high percentiles of response times, also known as *tail latencies*, are important because they directly affect users' experience of the service\n",
    "- optimizing for high percentiles can be hard and expensive as they are easily affected by random events outside of the control and the benefits are diminishing\n",
    "- percentiles are often used in *service level objectives* (SLOs) and *service level agreements* (SLAs), contracts that define the expected performance and availability of a service\n",
    "- queueing delays often account for a large part of the response time at high percentiles\n",
    "    - as a server can only process a small number of things in parallel (e.g., limited by # of CPU cores)\n",
    "    - *head-of-line blocking*: the server takes a small number of slow requests to hold up the processing of subsequent requests\n",
    "\n",
    "**Percentiles in Practice**\n",
    "- high percentiles become especially important in backend services that are called multiple times as part of serving a single end-user request\n",
    "    - it takes one slow call to make the entire end-user request slow\n",
    "    - even if only a small percentage of backend calls are slow, the chance of getting a slow call increases w/ the # of calls needed\n",
    "    - this is called *tail latency amplification*\n",
    "\n",
    "\n",
    "### Approaches for Coping with Load\n",
    "\n",
    "- ***scaling up***: vertical scaling, moving to a more powerful machine\n",
    "- ***scaling out***: horizontal scaling, distributing the load across multiple smaller machines\n",
    "- *shared-noting architecture*: distributing load across multiple machines\n",
    "- a system that can run on a single machine is often simpler, but high-end machines can become very expensive, so inensive workloads often cant avoid scaling out\n",
    "- *elastic* systems can automatically add computing resources when they detech a load increase\n",
    "- distributing stateless services across multiple machines is fairly straightforward, taking stateful data systems from a single node to a distributed setup can introduce lots of additional complexity\n",
    "    - common wisdom until recently was to keep the database on a single node (scale up) until scaling cost or high-availability requirements forced you to make it distributed\n",
    "    - it is conceivable that distributed data systems will become the default in the future, even for use cases that dont handle large volumnes of data or traffic, as tooling and abstactions for distributed systems get better\n",
    "- there is no generic, one-size-fits-all scalable architecture (*magic scaling sauce*)\n",
    "- an architecture that scales well for a particular application is built around assumptions of which operations will be common and which will be rare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maintainability\n",
    "\n",
    "- design software in such a way that it will hopefully minimize pain during maintenance, and thus avoid creating legacy software ourselves\n",
    "\n",
    "### Operability\n",
    "- make it easy for operations teams to keep the system running smoothly\n",
    "- operations responsibilities:\n",
    "    - monitoring the health of the system and restoring service if it goes into a bad state\n",
    "    - tracking down the cause of problems, such as system failures or degraded performance\n",
    "    - keeping software and platforms up to date, including security patches\n",
    "    - keeping tabs on how different systems affect each other, so that a problematic change can be avoided before it causes damage\n",
    "    - anticipating future problems and solving them before they occur (e.g., capacity planning)\n",
    "    - establishing good practices and tools for development, configuration management, and more\n",
    "    - performing complex maintenance tasks, such as moving an application from one platform to another\n",
    "    - defining processes that make operations predictable and help keep the production environment stable\n",
    "    - preserving the organization's knowledge about the system\n",
    "- data systems can do various things to make operation routine tasks easy\n",
    "    - providing **visibility into the runtime behavior and internals** of the system, with good monitoring\n",
    "    - providing good support for **automation and integration** with standard tools\n",
    "    - **avoiding dependency** on individual machines\n",
    "    - providing good **documentation** and an easy-to-understand operational model\n",
    "    - providing good **default behavior**, but also give admins the freedom to override defaults when needed\n",
    "    - self-healing where appropriate, but also giving admins manual control over the system state when needed\n",
    "    - exhibiting predicate behavior, minimizing surprises\n",
    "\n",
    "### Simplicity\n",
    "- make it easy for new engineers to understand the system, by removing as much complexity as possible from the system\n",
    "- various possible symptoms of complexity:\n",
    "    - explosion of the state space\n",
    "    - tight coupling of modules\n",
    "    - tangled dependencies\n",
    "    - inconsistent naming and terminology\n",
    "    - hacks aimed at solving performance problems\n",
    "    - special-casing to work around issues elsewhere\n",
    "    - ...\n",
    "\n",
    "### Evolvability\n",
    "- make it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
